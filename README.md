In these scripts, we train a simple nanoGPT model on the Tiny Shakespeare dataset on a MacBook Pro with an M4 Pro chip. We train the model for 5 epochs to a validation cross-entropy loss of around 1.85. Each epoch takes around 7 minutes, but I'm sure this can be optimized. 

Some example generated text is as follows:

```
PROGRAM:
Hello world! planent give her bountess;
His spoces which from selves my life,
And my made at fill mercy plague too,
The crome subjess with his no storms kings.

AUTOLYCUS:
Alack he him, taking this grace of Gloucester's sign.
Thou touch face is none leave the stand, here,
And, cousins, it's Dight. which, Me: we cannont not.

NORTHUMBERLAND:
Day, it poor Terrove; he PolX convey-mine;
My horsome comes wrung usurpvalt ard his grand:
And having of yourself well tear'dath of kneelss.

Lord:
Sour away! will, his bolds, but now the pay it were sperful?

Seconds Keymbowns.

VOLLUMBY:
Come, nor asknows. Hadk.

LEONTES:
Awails do
'The love thou hast gust, Catesby as myself.

GLOUCK:
I know them impaties.

MERCUTIO:
By the relokerate rages not,

Mattely do the bast deposard's story best?
But's a man'ther'd scold of Juliet myself whom,
I deserved in his plaint in himself.
If thou all, Moheford, honour too more to fear:
Say hope in cause aready, she find father'd.

TYFORK:
All I which, my lord, good doak mine deny.

GLOUCESTER:
Have chooler mother.

LADY CAPULET:
Why.
```

Of course, this is not a great model, but we were at least able to prototype it locally and fairly quickly.
