{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc74d1f-e2ec-4777-9ff4-5fb503dc58be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de20001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from loader import prepare_data\n",
    "from model import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055ecc25-88f5-4b95-bc19-cc9b992f0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"\n",
    "    Dataclass containing data, model, and training settings.\n",
    "    \"\"\"\n",
    "    train_frac:int = 0.7\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    block_size:int = 128\n",
    "    n_layer:int = 8\n",
    "    n_head:int = 8\n",
    "    n_embd:int = 32\n",
    "    batch_size:int = 128\n",
    "    weight_decay:float = 1e-4\n",
    "    learning_rate:float = 1e-3\n",
    "    n_epochs:int = 5\n",
    "    seed:int = 2024\n",
    "\n",
    "# Setup\n",
    "config = GPTConfig()\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(config.seed)\n",
    "\n",
    "# Dataloaders\n",
    "train_dataset, valid_dataset, tokenizer = prepare_data(config)\n",
    "config.vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e1929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = GPT(config)\n",
    "model.to(config.device)\n",
    "# Optimizer setup\n",
    "param_dict = {pn: p for pn, p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "# weight decay applied only to tensor weights of order >= 2\n",
    "decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "optim_groups = [\n",
    "    {\"params\": decay_params, 'weight_decay': config.weight_decay},\n",
    "    {\"params\": nodecay_params, \"weight_decay\": 0.0}\n",
    "]\n",
    "num_decay_params = sum(p.numel() for p in decay_params)\n",
    "num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "\n",
    "print(f\"Num decayed parameter tensors: {len(decay_params)}. Total decayed parameters: {num_decay_params}.\")\n",
    "print(f\"Num non-decayed parameter tensors: {len(nodecay_params)}. Total non-decayed parameters: {num_nodecay_params}.\")\n",
    "\n",
    "# AdamW Optimizer\n",
    "fused_avail = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "use_fused = fused_avail and 'mps' in config.device # torch supports mps for fused adamW\n",
    "print(f\"Using fused AdamW: {use_fused}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8df2e8-985e-4331-a712-f2a4e8ec5da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(optim_groups, lr=config.learning_rate, betas=(0.9,0.95), eps=1e-8, fused=use_fused)\n",
    "#optimizer = torch.optim.AdamW(optim_groups, lr=config.learning_rate, betas=(0.9,0.95), eps=1e-8)\n",
    "\n",
    "# Note, for simplicity, we will not use a LR scheduler. This may change in the future.\n",
    "\n",
    "# Training\n",
    "\n",
    "#@torch.compile\n",
    "def train_step(net:torch.nn.Module, context:torch.tensor, targets:torch.tensor):\n",
    "    optimizer.zero_grad()\n",
    "    logits = net(context)\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "#@torch.compile\n",
    "def valid_step(net:torch.nn.Module, context:torch.tensor, targets:torch.tensor):\n",
    "    logits = net(context)\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00edede0-99af-4cfd-b162-150c41a56718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print_freq = 100\n",
    "valid_freq = 1\n",
    "for epoch in range(config.n_epochs):\n",
    "    epoch_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch {epoch}/{config.n_epochs}, learning rate {epoch_lr}')\n",
    "\n",
    "    # Training step\n",
    "    model.train()\n",
    "    for step, (xb, yb) in enumerate(train_loader):\n",
    "        loss = train_step(model, xb, yb)\n",
    "\n",
    "        # Log training loss\n",
    "        if step % print_freq == 0:\n",
    "           print(f'Train Step {step}/{len(train_loader)} - Loss: {loss.item()}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    if epoch % valid_freq == 0:\n",
    "        valid_loss = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in valid_loader:\n",
    "                loss = valid_step(model, xb, yb)\n",
    "                valid_loss.append(loss.item())\n",
    "\n",
    "        # Log validation loss\n",
    "        val_loss = sum(valid_loss) / len(valid_loss)\n",
    "        print(f'Validation Loss: {val_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1302c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation\n",
    "# Generate some random text from the trained model.\n",
    "max_new_tokens = 10000\n",
    "past_tokens = torch.tensor(tokenizer.encode('PROGRAM:\\nHello world!')).to(config.device)\n",
    "model.eval()\n",
    "generated_tokens = model.generate(past_tokens[None,:], max_new_tokens)\n",
    "generated_text = tokenizer.decode(generated_tokens[0].tolist())\n",
    "\n",
    "with open('generated_text.txt', 'w') as file:\n",
    "    file.write(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
